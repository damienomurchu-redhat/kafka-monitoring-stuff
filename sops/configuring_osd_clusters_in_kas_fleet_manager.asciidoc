// begin header
ifdef::env-github[]
:tip-caption: :bulb:
:note-caption: :information_source:
:important-caption: :heavy_exclamation_mark:
:caution-caption: :fire:
:warning-caption: :warning:
endif::[]
:numbered:
:toc: macro
:toc-title: pass:[<b>Table of Contents</b>]
// end header
= Configure OSD Clusters in Kafka Service Fleet Manager

toc::[]

== Description

This SOP outlines the process of configuring data plane OSD clusters within kas-fleet-manager which includes adding/removing OSD clusters and marking a cluster as unschedulable.

== Prerequisites
* https://vault.devshift.net/ui/vault/secrets/app-interface/show/managed-service-api/production/service/credentials[Production AWS account ID and osdCcsAdmin credentials]
** aws.accountid
** aws.accesskey
** aws.secretaccesskey
* https://vault.devshift.net/ui/vault/secrets/app-interface/show/managed-service-api/production/service/credentials[OCM credentials to a user in the production organisation]
** ocm-service.clientId
** ocm-service.clientSecret
* https://stedolan.github.io/jq/download/[jq] installed

== Create the OSD cluster
1. Set the configuration
+
[source,sh]
----
export AWS_ACCOUNT_ID=<REPLACE_ME>
export AWS_ACCESS_KEY_ID=<REPLACE_ME>
export AWS_SECRET_ACCESS_KEY=<REPLACE_ME>
export NUM_COMPUTE_NODES=60
----
2. Login as the production service account
+
[source,sh]
----
ocm login --url=https://api.openshift.com --client-id=<client-id> --client-secret=<client-secret>
----
3. Create the OSD cluster
+
[source,sh]
----
cat << EOF | ocm post /api/clusters_mgmt/v1/clusters | jq ' .name, .id, .multi_az, .cloud_provider.id, .region.id '
{"name":"mk-$(date +'%m%d-%H%M%S')","region":{"id":"us-east-1"},"nodes":{"compute_machine_type":{"id":"m5.4xlarge"},"compute":$NUM_COMPUTE_NODES},"managed":true,"cloud_provider":{"id":"aws"},"multi_az":true,"node_drain_grace_period":{"value":60,"unit":"minutes"},"product":{"id":"osd"},"ccs":{"enabled":true,"disable_scp_checks":false},"aws":{"access_key_id":"$AWS_ACCESS_KEY_ID","account_id":"$AWS_ACCOUNT_ID","secret_access_key":"$AWS_SECRET_ACCESS_KEY"}}
EOF
----

== Add the OSD cluster to the kas-fleet-manager production configuration
1. Open an MR against the https://gitlab.cee.redhat.com/service/app-interface/-/blob/master/data/services/managed-services/cicd/saas/saas-kas-fleet-manager.yaml[kas-fleet-manager saas file] with the OSD cluster details
** https://gitlab.cee.redhat.com/service/app-interface/-/merge_requests/17711[Example MR as reference]
+
[source,yaml]
----
CLUSTER_LIST:
  - name: cluster-name
    cluster_id: 1jp6kdr7k0sjbe5adck2prjur8f39378
    cloud_provider: aws
    region: us-east-1
    multi_az: true
    schedulable: true # change this to false if you do not want the cluster to be schedulable
    kafka_instance_limit: 150 # change this to match any value of configuration
----
2. Ping the control plane team in the "MK - Control Plane" gchat room to get the MR reviewed and merged

== Marking an OSD cluster as unschedulable 
Unschedulable will stop any new Kafka instances being provisioned on this OSD cluster.

1. Open an MR against the https://gitlab.cee.redhat.com/service/app-interface/-/blob/master/data/services/managed-services/cicd/saas/saas-kas-fleet-manager.yaml[kas-fleet-manager saas file] and change the `schedulable` field to false for the correct OSD cluster

== Setting the maximum number of Kafkas per OSD cluster
1. Open an MR against the https://gitlab.cee.redhat.com/service/app-interface/-/blob/master/data/services/managed-services/cicd/saas/saas-kas-fleet-manager.yaml[kas-fleet-manager saas file] and change the `kafka_instance_limit` field to the number of Kafkas you would like the maximum to be for the correct OSD cluster

== Removing an OSD cluster from production
1. Verify all Kafka instances on the OSD cluster have been fully removed. The command below checks all namespaces for Kafka CR's:
+
----
oc get kafkas -A
----
2. Verify that the Cluster Logging operator add-on is no longer enabled. The following command should return a `404`:
+
----
ocm get /api/clusters_mgmt/v1/clusters/<cluster_id>/addons/cluster-logging-operator

// Expected output
{
  "kind": "Error",
  "id": "404",
  "href": "/api/clusters_mgmt/v1/errors/404",
  "code": "CLUSTERS-MGMT-404",
  "reason": "Add-on 'cluster-logging-operator' not enabled on cluster '<cluster_id>'",
  "operation_id": "<operation_id>"
}
----
+
If the operator add-on is enabled, the below command can be used to remove the add-on.
+
NOTE: You must be either the cluster owner or org admin to remove the add-on
+
----
ocm delete /api/clusters_mgmt/v1/clusters/<cluster_id>/addons/cluster-logging-operator
----
3. Modify the `CLUSTER_LIST` within the `ENVIRONMENT: production` section of the https://gitlab.cee.redhat.com/service/app-interface/-/blob/master/data/services/managed-services/cicd/saas/saas-kas-fleet-manager.yaml[kas-fleet-manager saas file] by removing the OSD cluster you want to delete from the list
4. Open an MR against with the changes from the previous step. Once the change has been verified, wait for it to merge, and then wait for the change to roll out to the production `kas-fleetmanager`
5. To verify that the roll out has been completed, check the config map on the APP-SRE cluster, which can be found https://console-openshift-console.apps.app-sre-prod-04.i5h0.p1.openshiftapps.com/k8s/ns/managed-services-production/configmaps/ocm-managed-services-dataplane-cluster-scaling-config/[here]. The cluster being removed should not be present in the `dataplane-cluster-configuration.yaml` section of the config map.
+
NOTE: Please note that is can take up to 30 minutes for the changes to be reflected in the config map
+
6. Verify that the OSD cluster is deleting through the OCM CLI or OCM UI


== Configuring cloud logging on the cluster
To install logging on the cluster the following https://github.com/bf2fc6cc711aee1a0c2a/kas-sre-sops/blob/main/sops/cluster_logging_operator_installation.asciidoc[SOP] is needed.

== Balancing the infra nodes
To balance the infra nodes preform the following https://github.com/bf2fc6cc711aee1a0c2a/kas-sre-sops/blob/master/sops/infra-node-pod-rebalancing.asciidoc#3-executeresolutionvalidate[SOP] when you are able to login with the IDP into the cluster as cluster-admin, otherwise you wont have permissions.

== Creating a read-only group
NOTE: This section requires a user with cluster-admin permissions to complete. Please ask one of the RTS team members to complete this step.

1. A new group with a read-only cluster role is required to facilitate users requesting access to the cluster. Users are added to this group using https://github.com/bf2fc6cc711aee1a0c2a/kas-sre-sops/blob/main/sops/configuring_osd_clusters_in_kas_fleet_manager.asciidoc[this] SOP.
+
Log in to the cluster using the API token and run the following command.
+
[source,sh]
----
oc adm groups new mk-readonly-access
----
+
2. Add the `dedicated-readers` cluster role to the group. This role allows only `get`, `list`, and `watch` permissions on resources, with users also unable to view secrets.
+
----
oc adm policy add-cluster-role-to-group dedicated-readers mk-readonly-access
----

== Assign the relevant users permissions
To assign permissions to user to be able to access the cluster follow this https://github.com/bf2fc6cc711aee1a0c2a/kas-sre-sops/blob/main/sops/adding_user_to_sso.asciidoc#32-giving-the-user-permission-in-osd-data-plane-cluster[SOP]

== Troubleshooting
None.
