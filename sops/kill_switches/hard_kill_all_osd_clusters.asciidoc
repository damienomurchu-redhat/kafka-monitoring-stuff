// begin header
ifdef::env-github[]
:tip-caption: :bulb:
:note-caption: :information_source:
:important-caption: :heavy_exclamation_mark:
:caution-caption: :fire:
:warning-caption: :warning:
endif::[]
:numbered:
:toc: macro
:toc-title: pass:[<b>Table of Contents</b>]
// end header
= Hard kill all OSD clusters

toc::[]

== Description
This SOP outlines how to immediately remove all data plane OSD clusters and their associated Kafka instances.

== Prerequisites
1. You have the `/teams/managed-services/roles/dev.yml` role added to your user in https://gitlab.cee.redhat.com/service/app-interface/-/tree/master/data/teams/managed-services/users[app-interface]. This is required to access the kas-fleet-manager dashboards and namespace.
** For example: https://gitlab.cee.redhat.com/service/app-interface/-/blob/master/data/teams/managed-services/users/dffrench.yml[dffrench.yml]
2. Fork the https://gitlab.cee.redhat.com/service/app-interface[app-interface] repository and add the https://gitlab.cee.redhat.com/devtools-bot[devtools-bot] user as a Maintainer of your fork.
3. Logged in as a user in the `Red Hat Kafka Service Fleet Manager` organisation (external_id of `14410147`) in OCM production. To verify you are logged in as a user in this org, run `ocm whoami | grep 14410147` and verify the external_id is shown.

== Execute/Resolution
1. Update the following properties and values under the `managed-services-production.yml` namespace parameters of the https://gitlab.cee.redhat.com/service/app-interface/-/blob/master/data/services/managed-services/cicd/saas/saas-kas-fleet-manager.yaml[kas-fleet-manager saas file].
** `KAFKA_CAPACITY_MAX_CAPACITY: 0`
** `KAFKA_LIFE_SPAN: 0`
** `CLUSTER_LIST: []`
2. Open an MR and contact one of the approvers listed by the @devtools-bot.
3. Contact the Incident Commanders, as per https://docs.google.com/document/d/1MyWj_V5QCFVHDjedo2OMeikslcMrf74nBC-kIATOh84/edit#, asking for the Status Page to be updated to say the Service is no longer available
4. Inform the `SREP Interrupt Catcher` user through the `sd-sre-platform` slack channel that they may see some alerts firing in `mk-` clusters, such as KubePersistentVolumeErrors. This is expected if the number of Kafka instances being torn down at the same time may cause the AWS API limits to be hit.

== Validate
1. Validate zero Kafka instances remain in any state from the https://grafana.app-sre.devshift.net/d/WLBv_KuMz/kas-fleet-manager-metrics?orgId=1[Kas Fleet Manager metrics dashboard].
2. Validate all OSD clusters have been successfully deleted from the https://cloud.redhat.com/openshift[OCM UI] or CLI.

== Troubleshooting
=== Kafka instances are taking too long to delete.
1. Ensure the Kafka instances are in a deprovision state from the https://grafana.app-sre.devshift.net/d/WLBv_KuMz/kas-fleet-manager-metrics?orgId=1[metrics dashboard]. 
2. If not, use the leader pods section of the same dashboard to get the name of the pod and check the pod logs from the https://console-openshift-console.apps.app-sre-prod-04.i5h0.p1.openshiftapps.com/k8s/cluster/projects/managed-services-production[namespace].
3. If the Kafkas are in a deprovision status, log into the data plane OSD cluster (via the Kafka SRE IdP) and verify the ManagedKafka CR and child resources have been removed. If not, investigate the kas-fleetshard-operator logs.

=== OSD clusters are taking too long to delete.
1. Contact SRE-P through the `sd-sre-platform` slack channel.